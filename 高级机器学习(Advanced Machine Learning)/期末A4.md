ch11 为什么特征选择: 降维, 去除不相关特征降低学习难度. 选特征子集: 前向搜索增加相关特征. 子集评价: 属性子集$A$, 信息增益$\operatorname{Gain}(A)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)$, 信息熵$\operatorname{Ent}(D)=-\sum_{i=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}$, $\operatorname{Gain}(A) \uparrow$ 则特征子集有助于分类的信息越多. 搜索+评价=特征选择, 前向搜+信息熵则类似决策树. 特征选择方法: 1 过滤式. Relief: 特征子集重要性: 子集中每个特征所对应的相关统计量分量$\delta^{j}=\sum_{i}-\operatorname{diff}\left(x_{i}^{j}, x_{i, \mathrm{nh}}^{j}\right)^{2}+\operatorname{diff}\left(x_{i}^{j}, x_{i, \mathrm{nm}}^{j}\right)^{2}$之和, 选阈值$\tau$以上的. $\boldsymbol{x}_{i, \mathrm{nm}}$ 猜错近邻, 属性有益则$\boldsymbol{x}_{i}$与$\boldsymbol{x}_{i, \mathrm{nm}}$距离大于$\boldsymbol{x}_{i}$与$\boldsymbol{x}_{i, \mathrm{nh}}$. 2 包裹式. LVM拉斯维加斯方法框架下随机搜索出特征子集, 训练后看误差. 3 嵌入式(特征选择与训练过程融合). LASSO, $\mathbf{L}_1$范数. 近端梯度下降PGD: $\nabla f$ 满足L-Lipschitz. 迭代 $\boldsymbol{x}_{k+1}=\underset{\boldsymbol{x}}{\arg \min } \frac{L}{2}\left\|\boldsymbol{x}-\left(\boldsymbol{x}_{k}-\frac{1}{L} \nabla f\left(\boldsymbol{x}_{k}\right)\right)\right\|_{2}^{2}+\lambda\|\boldsymbol{x}\|_{1}$, 闭式解 $x_{k+1}^{i}=z^{i}-\lambda / L, \lambda / L < z^i; \  z^{i}+\lambda / L, z^i < -\lambda/L; \ 0 \text{ 其他}$. 稀疏表示与字典学习: 优点 1 使大多数问题线性可分, 2 存储负担小. 字典学习: 为稠密表达找字典$\rightarrow$稀疏表达: 1 固定$\mathbf{B}$ 像LASSO解法: $\min _{\boldsymbol{\alpha}_{i}}\left\|\boldsymbol{x}_{i}-\mathbf{B} \boldsymbol{\alpha}_{i}\right\|_{2}^{2}+\lambda\left\|\boldsymbol{\alpha}_{i}\right\|_{1}$, 2 以$\boldsymbol{\alpha}_i$为初值 $\min _{\mathbf{B}}\|\mathbf{X}-\mathbf{B A}\|_{F}^{2} =\min _{\boldsymbol{b}_{i}}\left\|\mathbf{X}-\sum_{j=1}^{k} \boldsymbol{b}_{j} \boldsymbol{\alpha}^{j}\right\|_{F}^{2}$ $=\min _{\boldsymbol{b}_{i}}\left\|(\mathbf{X}-\sum_{j \neq i} \boldsymbol{b}_{j} \boldsymbol{\alpha}^{j})-\boldsymbol{b}_{i} \boldsymbol{\alpha}^{i}\right\|_{F}^{2}$ 括号内$\mathbf{E}_i$进行奇异值分解, 但直接分解可能破坏$\mathbf{A}$的稀疏性, $\therefore$ KSVD对$\boldsymbol{\alpha}_i$保留非零元素 $\mathbf{E}_i$仅保留$\boldsymbol{b}_i$与$\boldsymbol{\alpha}_i$的非零元素乘积项. 压缩感知 $\boldsymbol{y}= \boldsymbol{\Phi} \boldsymbol{x} = \boldsymbol{\Phi} \boldsymbol{\Psi} \boldsymbol{s}=\mathbf{A} \boldsymbol{s}$ 恢复$\boldsymbol{s}$$\Rightarrow$恢复$\boldsymbol{x}$, $k$限定等距性k-RIP $\left(1-\delta_{k}\right)\|\boldsymbol{s}\|_{2}^{2} \leqslant\left\|\mathbf{A}_{k} \boldsymbol{s}\right\|_{2}^{2} \leqslant\left(1+\delta_{k}\right)\|\boldsymbol{s}\|_{2}^{2}$ 此时 $\min _{\boldsymbol{s}}\|\boldsymbol{s}\|_{0} \Rightarrow \|\boldsymbol{s}\|_{1} $ s.t. $ \boldsymbol{y}=\mathbf{A} \boldsymbol{s}$. 矩阵补全 $\min _{\mathbf{X}} \operatorname{rank}(\mathbf{X})$ s.t. $(\mathbf{X})_{i j}=(\mathbf{A})_{i j}, (i, j) \in \Omega$, 转化: $\because \operatorname{rank} (\mathbf{X})$在集合$\left\{\mathbf{X} \in \mathbb{R}^{m \times n}:\|\mathbf{X}\|_{F}^{2} \leqslant 1\right\}$上的凸包是$\mathbf{X}$的“核范数" $\|\mathbf{X}\|_{*}=\sum_{j=1}^{\min \{m, n\}} \sigma_{j}(\mathbf{X})$.

ch12 学习算法$\mathfrak{L}$学得的模型对应的假设$h$尽可能接近目标概念$c$. 不能完全一致因为$D$有限$D$采样偶然. 1PAC辨识: $\mathfrak{L}$从$\mathcal{H}$中PAC辨识$\mathcal{C}$: $P(E(h) \leqslant \epsilon) \geqslant 1-\delta$. $\mathcal{C}$ 2PAC可学习: $\mathfrak{p} = \operatorname{poly}(1 / \epsilon, 1 / \delta, \operatorname{size}(\boldsymbol{x}), \operatorname{size}(c))$ $\forall \ m \geqslant \mathfrak{p}$ 可PAC辨识. 3PAC学习算法: $\mathfrak{L}$ PAC可学习且运行时间$=\mathfrak{p}$. 4样本复杂度: PAC学习算法最小$m \geqslant \mathfrak{p}$. 有限假设空间 可分: $\mathfrak{L}$以概率$1 - \delta$找到目标假设的$\epsilon$近似: $\text{All} \ i, P(h(\boldsymbol{x}_i = y_i))<(1 - \epsilon)^m$. 对$\text{All} \ h$, 上式$< | \mathcal{H} | (1-\epsilon)^m < |\mathcal{H}| e^{-m\epsilon} \leqslant \delta$, 上式可推$m \geqslant \cdots$. 不可分: $\forall h$ $P\left(|E(h)-\widehat{E}(h)| \leqslant \sqrt{({\ln |\mathcal{H}|+\ln (2 / \delta)})/ ({2 m}})\right) \geqslant 1-\delta$, 5不可知PAC可学习: $\forall \ m \geqslant \mathfrak{p}$ $P\left(E(h)-\min _{h^{\prime} \in \mathcal{H}} E\left(h^{\prime}\right) \leqslant \epsilon\right) \geqslant 1-\delta$. 6增长函数: $\Pi_{\mathcal{H}}(m)=\max _{\left\{\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{m}\right\} \subseteq \mathcal{X}}$ $\left|\left\{\left(h\left(\boldsymbol{x}_{1}\right), \ldots, h\left(\boldsymbol{x}_{m}\right)\right) \mid h \in \mathcal{H}\right\}\right|$ 集合的数量. 7对分: $\mathcal{H}$中的$h$对$D$中赋予标记的每种可能结果. 8$D$被$\mathcal{H}$打散: 二分类问题$\Pi_{\mathcal{H}}(m)=2^{m}$ 实现$\forall$对分. 9$\text{VC}$维: $\operatorname{VC}(\mathcal{H})=\max \left\{m: \Pi_{\mathcal{H}}(m)=2^{m}\right\}$. VC维$d$: $\exists$大小为$d$的示例集能被$\mathcal{H}$打散, $\forall$大小为$d+1$的示例集不能. 

VC维$= d$, 表示: 任意$^1$ $\forall$大小为$d+1$的示例集不能被$\mathcal{H}$打散$^2$.

VC维 定义, 双层量词: 如上 $^1 \ ^2$ 位置, $^2$ 处的量词是 $\forall$ 对分都能被$\mathcal{H}$实现. 

利用增长函数估计$E(h)$和$\widehat{E}(h)$之间的关系: 